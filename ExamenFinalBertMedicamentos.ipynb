{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LINK del Repositorio: https://github.com/WilberBarcayaM/SIS421-02-23"
      ],
      "metadata": {
        "id": "2XlnsTkzgNRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8hOHq8gGBgX",
        "outputId": "ce1508f9-ce58-47f6-bfda-d8a1881612ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLnkza3PGEtX",
        "outputId": "a06e4b2c-fafa-4d8b-e99d-534f27b483a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset #Esta función se utiliza para cargar conjuntos de datos\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration # Estas clases son parte de la biblioteca Transformers de Hugging Face y se utilizan para trabajar con el modelo BART\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader #Estas clases son esenciales para manejar conjuntos de datos y crear dataloaders para el entrenamiento de modelos\n",
        "from tqdm import tqdm #se utiliza para mostrar barras de progreso durante el entrenamiento\n"
      ],
      "metadata": {
        "id": "LqdjckVSYkN-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el conjunto de datos\n",
        "dataset = load_dataset(\"Areeb123/drug_reviews\")\n",
        "\n",
        "# Limitar el porcentaje de datos después de cargar el conjunto de datos\n",
        "percentage = 20  # ajusta según sea necesario\n",
        "dataset['train'] = dataset['train'].shuffle(seed=42).select([i for i in range(int(len(dataset['train'])*percentage/100))])\n",
        "dataset['test'] = dataset['test'].shuffle(seed=42).select([i for i in range(int(len(dataset['test'])*percentage/100))])\n",
        "dataset['validation'] = dataset['validation'].shuffle(seed=42).select([i for i in range(int(len(dataset['validation'])*percentage/100))])\n",
        "\n",
        "# Seleccionar las columnas relevantes\n",
        "selected_columns = ['drugName', 'review']\n",
        "dataset = dataset.select_columns(selected_columns)\n",
        "\n",
        "# Acceder a las primeras 5 filas del conjunto de datos\n",
        "primeras_filas = dataset['train'][:5]\n",
        "print(\"Primeras 5 filas:\")\n",
        "print(primeras_filas)\n",
        "\n",
        "\n",
        "# Crear instancia del tokenizer y del modelo BART\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\") #se encarga de dividir el texto en unidades más pequeñas llamadas \"tokens\" y asignarles identificadores numéricos únicos\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\") #Se crea una instancia del modelo BART para generación condicional. Este modelo está preentrenado en tareas de generación de texto y puede ser afinado para tareas específicas,\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VTiZ_f7YmJd",
        "outputId": "2410c9d6-893b-4058-9763-bff37b55430a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 5 filas:\n",
            "{'drugName': ['Lamotrigine', 'Lysine', 'Varenicline', 'Bisacodyl', 'Donepezil'], 'review': ['\"I started on Lamictal after having manic episodes on Zoloft as well as Wellbutrin. I was a heavy cigarette and marijuana smoker. In the first couple weeks of starting Lamictal, I was able to quit smoking weed and cigarettes!! I didn\\'t have insomnia as I previously did when trying to quit marijuana. My mood was much improve. I had been suicidal before and I started exercising again on Lamictal. I developed an unknown rash about 2 years into Lamictal and stopped immediately. The rash disappeared. It looked like 8 mosquito bites all over my legs. I was forced to try Abilify which made me gain over 40 pounds and take naps during the day. Today, I have started back on Lamictal and am back on track with exercising and enjoying life.\"', '\"There is no cure for HSV2 but you can suppress it with Lysine. You have to take it everyday and it just becomes part of your daily routine, like brushing your teeth or taking a shower. \\r\\n\\r\\nI was diagnosed with HSV2 in 2012 and had outbreaks every single month for the first year.  After that it was about every other month. It was a painful and humiliating experience for years. After introducing Lysine, along with OLE into my daily routine, I haven\\'t had an outbreak since September 2015. If I feel like an outbreak may occur, I just up the dosage until that familiar tingling sensation goes away. \\r\\n\\r\\nI know Lysine doesn\\'t work for everyone, but I stand by it 100% for my HSV2 suppression.\"', '\"March 26 2013, I will be smoke free for 3 years because of Chantix. I started Chantix \"Valentines\" Feb 14th and on March 26th I had 3 cigarettes left, I told my son I was going to smoke them back to back and never buy a pack again. I won!  Chantix was a real help but it holds true you can take anything to stop smoking but if the willingness of wanting to really want to quit is not there...you\\'re not going to stop smoking. Chantix does help curve that urge to use but you still have to use your own  restraint and increase that restraint daily to fulfill your quest of stopping. Chantix itself will not make you quit...it is an aid in quitting as crutches are an aid till your broken leg heals.\"', '\"Bisacodyl should be taken on an empty stomach.  It has a capsule that protects the medicine from acting on the stomach.  If you take it with food or close to a meal, then there it will open in the stomach and you will get the bloating and nausea.  Suppository is a great alternative and it works very well.  For me it started working within 30 minutes.\"', '\"My 65 yo husband started the 1/2 pill regiment 5 weeks ago. Big improvement in conversations, no longer accusing people of stealing from him. Than we had to wait 5 days for the prescription to be refilled. That was a decline, back to talking to imaginary people, aggressive behavior, no energy. Now he is on the whole pill. Wow, almost back to being pleasant. He takes his in the morning, so no sleep interrupted. Energy back, never had a side affect as far as loss of appetite, weight loss, diarrhea. Early stage of ALZ/ Dementia. Glad he is doing better, still a day to day.\"']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir tu conjunto de datos como antes\n",
        "\n",
        "class DrugRecommendationDataset(Dataset): #servirá para manejar y cargar los datos\n",
        "    def __init__(self, dataset, tokenizer, max_length): #Este es el método constructor de la clase. Se llama cuando se crea una nueva instancia de la clase\n",
        "        self.dataset = dataset #Almacena el conjunto de datos\n",
        "        self.tokenizer = tokenizer #Almacena la instancia del tokenizador\n",
        "        self.max_length = max_length #Almacena la longitud máxima para el padding\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) #devuelve la longitud total del conjunto de datos\n",
        "\n",
        "    def __getitem__(self, index): #Este método se utiliza para obtener un elemento del conjunto de datos en una posición específica index\n",
        "        #Se extraen los textos de síntomas y el nombre del medicamento para el índice dado\n",
        "        symptoms_text, drug_name = self.dataset['review'][index], self.dataset['drugName'][index]\n",
        "\n",
        "        input_text = f\"Recomendar medicamento para: {symptoms_text}\"\n",
        "        target_text = drug_name #El texto objetivo es simplemente el nombre del medicamento\n",
        "        inputs = self.tokenizer( #Se utiliza el tokenizador para convertir el texto de entrada en una representación numérica que el modelo pueda entender\n",
        "            input_text,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "        )\n",
        "        targets = self.tokenizer( #se tokeniza el texto objetivo nombre del medicamento\n",
        "            target_text, #Es el texto que se desea tokenizar y convertir en una representación numérica.\n",
        "            max_length=self.max_length, #Especifica la longitud máxima de la secuencia de salida después del tokenización\n",
        "            return_tensors=\"pt\", #Indica que se quiere que el resultado sea un tensor de PyTorch\n",
        "            padding=\"max_length\", #Especifica que se desea realizar el padding hasta la longitud máxima especificada\n",
        "            truncation=True, #Indica que se debe truncar la secuencia si excede la longitud máxima especificada. Esto es útil para asegurar que todas las secuencias tengan la misma longitud.\n",
        "        )\n",
        "        return { #Se devuelve un diccionario con tres claves \"nput_ids, attention_mask, y labels\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),#Contiene los identificadores numéricos de los tokens de la secuencia de entrada, después de ser procesados por el tokenizador. En este caso, se usa .squeeze() para eliminar cualquier dimensión adicional y obtener un tensor unidimensional\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(), #Es una máscara de atención que indica qué elementos de la secuencia de entrada son tokens reales y cuáles son tokens de padding\n",
        "            \"labels\": targets[\"input_ids\"].squeeze(), #Contiene los identificadores numéricos de los tokens de la secuencia objetivo etiqueta después de ser procesados por el tokenizador\n",
        "        }\n"
      ],
      "metadata": {
        "id": "5BdUmMSbYoo8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Crear instancia de la clase DrugRecommendationDataset\n",
        "dataset_instance = DrugRecommendationDataset(dataset['train'], tokenizer, max_length=32)  # Ajusta max_length según sea necesario\n",
        "\n",
        "# Crear el dataloader\n",
        "batch_size = 16\n",
        "dataloader = DataLoader(dataset_instance, batch_size=batch_size, shuffle=True) #sfuflle  garantiza que los lotes se generen en orden aleatorio en cada época de entrenamiento\n"
      ],
      "metadata": {
        "id": "SjgYBb83Yu-F"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Definir la función fit\n",
        "def fit_bart(model, dataloader, epochs=1, device=\"cuda\"):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) #Se utiliza el optimizador AdamW para ajustar los pesos del modelo. La tasa de aprendizaje se establece en 1e-5\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0 #Variable para acumular la pérdida durante el entrenamiento.\n",
        "        bar = tqdm(dataloader) #Variable para acumular la pérdida durante el entrenamiento.\n",
        "\n",
        "        for batch in bar:\n",
        "            inputs = { #Prepara los datos de entrada para el modelo moviéndolos al dispositivo.\n",
        "                \"input_ids\": batch[\"input_ids\"].to(device),  #Accede a la información de los identificadores de entrada en el lote actual.\n",
        "                \"attention_mask\": batch[\"attention_mask\"].to(device), #Accede a la máscara de atención asociada al lote actual.\n",
        "                \"labels\": batch[\"labels\"].to(device), #Accede a las etiquetas asociadas al lote actual\n",
        "            }\n",
        "\n",
        "            outputs = model(**inputs) #Realiza el cálculo hacia adelante forward utilizando el modelo.\n",
        "            loss = outputs.loss #Obtiene la pérdida del modelo para el lote actual.\n",
        "\n",
        "            optimizer.zero_grad() # Limpia los gradientes acumulados en los parámetros\n",
        "            loss.backward() #Calcula los gradientes retropropagando la pérdida.\n",
        "            optimizer.step() #Actualiza los parámetros del modelo utilizando el optimizador.\n",
        "\n",
        "            train_loss += loss.item() #Acumula la pérdida del lote actual.\n",
        "\n",
        "            bar.set_postfix(epoch=epoch, loss=train_loss) #Actualiza la información mostrada en la barra de progreso con el número de época y la pérdida acumulada\n"
      ],
      "metadata": {
        "id": "nGULg5RvGKrv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo BART\n",
        "fit_bart(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhZtvITGGRbN",
        "outputId": "23736c58-27d3-404d-d0e5-56b967e11ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1386/1386 [1:45:35<00:00,  4.57s/it, epoch=0, loss=1.57e+3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_medication_bart(model, symptoms_text, tokenizer, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    input_text = f\"Recomendar medicamento para: {symptoms_text}\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        max_length=64,  # Ajusta según sea necesario\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    outputs = model.generate(**inputs, max_length=64)  # Ajusta según sea necesario\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded_output\n",
        "\n"
      ],
      "metadata": {
        "id": "hfnb91oliI9g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de predicción\n",
        "symptoms_text = \"Experiencing muscle pain, fatigue, and a persistent cough.\"\n",
        "prediction = predict_medication_bart(model, symptoms_text, tokenizer, device=\"cuda\")\n",
        "print(\"Recomendación de medicamento:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al7RzV4zI6r2",
        "outputId": "261e8ee9-c4fb-4d5c-ab2e-3d7c4fff17e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recomendación de medicamento: Bupropion / naltrexone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# guardar modelo\n",
        "\n",
        "PATH = '/content/drive/MyDrive/chechpoint/checkpoint.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "HcAOY9QyjgPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bart(model, dataloader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bar = tqdm(dataloader)\n",
        "\n",
        "        for batch in bar:\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
        "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
        "                \"labels\": batch[\"labels\"].to(device),\n",
        "            }\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            bar.set_postfix(loss=total_loss)\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# # Uso\n",
        "# val_loss = evaluate_bart(model, val_dataloader)\n",
        "# print(f\"Validation Loss: {val_loss}\")\n"
      ],
      "metadata": {
        "id": "_ZunndJI6E0h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/chechpoint/checkpoint.pt'"
      ],
      "metadata": {
        "id": "NB4zebSl6Auy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar modelo\n",
        "\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "LUkZQ8yekhwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67483084-a9b2-4027-8235-2c2a5ffac86f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fit_bart(model, dataloader, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTBwqBRP6BqT",
        "outputId": "fcdaa881-b4d4-44c0-e8b4-7da2da3cdfe0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1386/1386 [1:46:20<00:00,  4.60s/it, epoch=0, loss=390]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# guardar modelo\n",
        "\n",
        "PATH = '/content/drive/MyDrive/chechpoint/checkpoint2.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "4aw0ijcs6euM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de predicción\n",
        "symptoms_text = \"Experiencing muscle pain, fatigue, and a persistent cough.\"\n",
        "prediction = predict_medication_bart(model, symptoms_text, tokenizer, device=\"cuda\")\n",
        "print(\"Recomendación de medicamento:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6q8Sos9TANV",
        "outputId": "7f598c44-969e-434a-f21f-4d02560cec98"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recomendación de medicamento: Phentermine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_t6mgP5TJfN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}